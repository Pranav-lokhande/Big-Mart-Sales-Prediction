# -*- coding: utf-8 -*-
"""Big-Mart-Sales-III.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jP9Yyc-OgcXVYNWmVVrfUJfe7Yj2V09B
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

Sales_train = pd.read_csv("/content/train_v9rqX0R.csv")
Sales_train["Type"] = "train"
print(Sales_train.shape)

Sales_test = pd.read_csv("/content/test_AbJTz2l.csv")
Sales_test["Type"] = "test"
print(Sales_test.shape)

Data_Predicted = Sales_test.loc[:,["Item_Identifier","Outlet_Identifier","Item_MRP"]]  ## will use later while prediction
#Data_Predicted.head()

dframes = [Sales_train,Sales_test]
Sales = pd.concat(dframes,ignore_index = True)
print(Sales.shape)
Sales.tail()

Sales.describe(include="all")

"""## Visibility can't be 0
###### hence imputing it
"""

Sales["Item_Visibility"].describe()

UniqueItems = set(Sales.Item_Identifier)
for each in UniqueItems:
    Sales.loc[(Sales["Item_Identifier"]==str(each)) & (Sales["Item_Visibility"]==0),"Item_Visibility"] = Sales.loc[(Sales["Item_Identifier"]==str(each)) & (Sales["Item_Visibility"]!=0),"Item_Visibility"].mean()
Sales["Item_Visibility"].describe()

"""### Adding new feature Quantity Sold"""

Sales["Qty_Sold"] = (Sales["Item_Outlet_Sales"]/Sales["Item_MRP"])
Sales.head()

Sales.isnull().sum()

"""### To impute "Item_Weight", we are filtering the data for each unique "Item_Identifier" and then assigning the mean value of that category to the one which is "NA"
"""

for each in UniqueItems:
    Sales.loc[(Sales["Item_Identifier"]==str(each)) & (Sales["Item_Weight"].isnull()),"Item_Weight"] = Sales.loc[Sales["Item_Identifier"]==str(each),"Item_Weight"].mode()[0]

"""### cheking the imputation"""

Sales.Item_Weight.isnull().sum()

"""### Changing the Year Establishment year to Age"""

import datetime
now = datetime.datetime.now()
now.year

Sales["Outlet_Age"] = now.year - Sales["Outlet_Establishment_Year"]
Sales["Outlet_Age"].head()



import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Ensure only numeric columns are used for correlation
numeric_sales = Sales.select_dtypes(include=['number'])

plt.figure(figsize=(10, 7))
sns.heatmap(numeric_sales.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

Sales.describe(include=["object"]).columns

Sales.Outlet_Size.value_counts()

Sales.loc[Sales["Outlet_Size"].isnull(),"Outlet_Identifier"].value_counts()



"""### Looking for pattern for the missing Outlet_size"""

Sales.loc[(Sales["Outlet_Size"].isnull()) & (Sales["Outlet_Identifier"] == "OUT045") ,].describe(include = [object])

Sales.loc[(Sales["Outlet_Size"].isnull()) & (Sales["Outlet_Identifier"] == "OUT017") ,].describe(include = [object])

Sales.loc[(Sales["Outlet_Size"].isnull()) & (Sales["Outlet_Identifier"] == "OUT010") ,].describe(include = [object])

"""### As for OUT045 and OUT017; Outlet_Location_Type	and Outlet_Type are same i.e. Tier 2 and Supermarket Type1, hence we can impute the data as per this"""

Sales.loc[(Sales["Outlet_Location_Type"]== "Tier 2") & (Sales["Outlet_Type"]=="Supermarket Type1") ,"Outlet_Size"].value_counts()

Sales.loc[(Sales["Outlet_Size"].isnull()) & (Sales["Outlet_Identifier"].isin(["OUT045","OUT017"])) ,"Outlet_Size"] = "Small"
Sales.loc[ (Sales["Outlet_Identifier"].isin(["OUT045","OUT017"])) ,"Outlet_Size"].value_counts()

"""## for "OUT010"
"""

Sales.loc[(Sales["Outlet_Location_Type"]== "Tier 3") & (Sales["Outlet_Type"]=="Grocery Store") ,"Outlet_Size"].value_counts()

Sales.loc[(Sales["Outlet_Type"]=="Grocery Store") ,"Outlet_Size"].value_counts()

Sales.loc[(Sales["Outlet_Location_Type"]== "Tier 3") ,"Outlet_Size"].value_counts()

Sales.loc[Sales["Outlet_Size"].isnull(),].shape

Sales['Item_Type'].value_counts()

"""### To impute "Outlet_Size for OUT010", we will build a classifier

### Changing the categorical data to numeric/dummy

##### imputing ordinal data
"""

mapping_Item_Type = {'Fruits and Vegetables': "Fruit_Veg",
                    'Household':"HH_HH", 'Health and Hygiene':"HH_HH",
                    'Baking Goods': "Bake_Snacks", 'Snack Foods': "Bake_Snacks",
                    'Canned': "Frozen_Canned", 'Frozen Foods': "Frozen_Canned",
                    'Dairy': "DBBS", 'Breakfast': "DBBS", 'Breads': "DBBS", 'Starchy Foods': "DBBS",
                    'Seafood':"Seafood_Meat", 'Meat': "Seafood_Meat",
                    'Hard Drinks': "Drinks", 'Soft Drinks': "Drinks",
                    'Others':"Others"}
Sales['Item_Type'] = Sales['Item_Type'].map(mapping_Item_Type)

mapping_Item_Fat_Content = {'Regular': 1, "reg": 1,'LF': 0,'Low Fat': 0, "low fat":0}
Sales['Item_Fat_Content'] = Sales['Item_Fat_Content'].map(mapping_Item_Fat_Content)

mapping_Outlet_Size = {'Small': 1, "Medium": 2,'High': 3}
Sales['Outlet_Size'] = Sales['Outlet_Size'].map(mapping_Outlet_Size)

Sales.head()

"""##### imputing nominal data"""

Sales = pd.get_dummies(Sales,columns=["Item_Type","Outlet_Identifier","Outlet_Location_Type","Outlet_Type"],drop_first=True)

Sales.head()

Sales.columns

X_Cols = ['Item_Fat_Content', 'Item_MRP',
       'Item_Visibility', 'Item_Weight',
       'Qty_Sold', 'Outlet_Age', 'Item_Type_DBBS',
       'Item_Type_Drinks', 'Item_Type_Frozen_Canned', 'Item_Type_Fruit_Veg',
       'Item_Type_HH_HH', 'Item_Type_Others', 'Item_Type_Seafood_Meat',
       'Outlet_Identifier_OUT013', 'Outlet_Identifier_OUT017',
       'Outlet_Identifier_OUT018', 'Outlet_Identifier_OUT019',
       'Outlet_Identifier_OUT027', 'Outlet_Identifier_OUT035',
       'Outlet_Identifier_OUT045', 'Outlet_Identifier_OUT046',
       'Outlet_Identifier_OUT049', 'Outlet_Location_Type_Tier 2',
       'Outlet_Location_Type_Tier 3', 'Outlet_Type_Supermarket Type1',
       'Outlet_Type_Supermarket Type2', 'Outlet_Type_Supermarket Type3']
y_cols = 'Outlet_Size'

from sklearn.model_selection import train_test_split
X = Sales.loc[(Sales[y_cols].notnull()) & (Sales['Type'] == "train"), X_Cols]
y = Sales.loc[(Sales[y_cols].notnull()) & (Sales['Type'] == "train"), y_cols]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=20)



print("Shape of X_train: ",X_train.shape)
print("Shape of y_train: ",y_train.shape)
print("Shape of X_test: ",X_test.shape)
print("Shape of y_test: ",y_test.shape)

Sales[y_cols].value_counts()



"""### As there are 3 categories, we can use all the classificaition techniques except Logistic Regression

## Trying K-Nearest Neighbours
"""

from sklearn import metrics
from sklearn.neighbors import KNeighborsClassifier
KNeighborsClassifier()

def IdentifyKValueCrossValidation(X,Y,startK,endK,cv,scoring):
    k_range = list(range(startK, endK+1))
    k_scores = []
    for k in k_range:
        knn = KNeighborsClassifier(n_neighbors=k)
        scores = cross_val_score(knn, X, Y, cv=cv, scoring=scoring)
        k_scores.append(scores.mean())
    z = [i for i, j in enumerate(k_scores) if j == max(k_scores)]

    print("Location for Max Accuaracy is:")

    for i in z:
        print(k_range[i])

    # plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)
    plt.plot(k_range, k_scores)
    plt.xlabel('Value of K for KNN')
    plt.ylabel('Cross-Validated Accuracy')

    return k_range[i]

def metrices(Predicted,Actual):
    print("Confusion Matrix for the model is:\n\n {}".format(metrics.confusion_matrix(y_pred=Predicted,y_true=Actual)))
    print("\nAccuracy for the model is: {}".format(metrics.accuracy_score(y_pred=Predicted, y_true= Actual)))
    print("\nArea under the curve for the model is: {}".format(metrics.roc_auc_score(y_score=Predicted,y_true=Actual)))
    print("\nClassification Report for the model is:\n {}".format(metrics.classification_report(y_pred=Y_Predict,y_true=y_test)))

from sklearn.model_selection import cross_val_score
K = IdentifyKValueCrossValidation(X=X,Y=y,cv=5,startK=1,endK=50,scoring="accuracy")
print("Value of K with is: {}".format(K))

knn = KNeighborsClassifier(n_neighbors=K)
y_predicted = knn.fit(X_train,y_train).predict(X_test)
print("Test Accuracy: ", (y_predicted == y_test).astype(int).sum()/y_test.shape[0])

pd.Series(knn.predict(X=Sales.loc[(Sales[y_cols].isnull()) & (Sales['Type'] == "train"), X_Cols])).value_counts()



"""### Multinomial Classifier"""

from sklearn.multiclass import OneVsOneClassifier
from sklearn.svm import LinearSVC

md = OneVsOneClassifier(LinearSVC(random_state=0)).fit(X_train, y_train)

y_predicted = md.predict(X=X_test)
print("Test Accuracy: ", (y_predicted == y_test).astype(int).sum()/y_test.shape[0])
#pd.Series(y_predicted).value_counts()

pd.Series(md.predict(X=Sales.loc[(Sales[y_cols].isnull()) & (Sales['Type'] == "train"), X_Cols])).value_counts()



"""### AdaBoost"""

from sklearn.ensemble import AdaBoostClassifier
clf = AdaBoostClassifier(n_estimators=100)
scores = cross_val_score(clf, X_train, y_train)
scores.mean()

clf.fit(X_train, y_train)

pd.Series(clf.predict(X=Sales.loc[(Sales[y_cols].isnull()) & (Sales['Type'] == "train"), X_Cols])).value_counts()





"""### As majority of the predictions for Multinomial and AdaBoost was 100% accuracy were Medium, hence we will impute accordingly"""

Sales.loc[(Sales["Outlet_Size"].isnull()),"Outlet_Size"] = 2
Sales["Outlet_Size"].value_counts()

Sales["Outlet_Size"] = Sales["Outlet_Size"].astype(int).astype(object)
Sales["Outlet_Size"].value_counts()

Sales.isnull().sum()

"""## Hurray! Imputing is DONE!"""

from sklearn import metrics
from sklearn.metrics import r2_score

Sales.columns

XCols = ['Item_Fat_Content', 'Item_MRP',
       'Item_Visibility', 'Item_Weight',
       'Outlet_Size', 'Outlet_Age', 'Item_Type_DBBS',
       'Item_Type_Drinks', 'Item_Type_Frozen_Canned', 'Item_Type_Fruit_Veg',
       'Item_Type_HH_HH', 'Item_Type_Others', 'Item_Type_Seafood_Meat',
       'Outlet_Identifier_OUT013', 'Outlet_Identifier_OUT017',
       'Outlet_Identifier_OUT018', 'Outlet_Identifier_OUT019',
       'Outlet_Identifier_OUT027', 'Outlet_Identifier_OUT035',
       'Outlet_Identifier_OUT045', 'Outlet_Identifier_OUT046',
       'Outlet_Identifier_OUT049', 'Outlet_Location_Type_Tier 2',
       'Outlet_Location_Type_Tier 3', 'Outlet_Type_Supermarket Type1',
       'Outlet_Type_Supermarket Type2', 'Outlet_Type_Supermarket Type3']
YCols = 'Qty_Sold'

XX = Sales.loc[Sales["Type"]=="train",XCols]
yy = Sales.loc[Sales["Type"]=="train",YCols]
print(XX.shape)
print(yy.shape)
XX_train, XX_test, yy_train, yy_test = train_test_split(XX, yy, test_size=0.3, random_state=5)

"""### Linear Regression with all the columns"""

from sklearn import linear_model

# Remove normalize=True ✅
reg = linear_model.LinearRegression(fit_intercept=True)
reg.fit(XX_train, yy_train)

print("Intercept:", reg.intercept_)
print("Coefficients:", reg.coef_)

print(len(reg.coef_))
len(XCols)

coef1 = pd.DataFrame(reg.coef_,XCols,columns=["Value"])
#coef1[coef1["Value"]>0].sort_values(by="Value",ascending=False)

yy_predicted = reg.predict(XX_test)
metrics.mean_squared_error(y_true=yy_test, y_pred=yy_predicted)

r2_score(yy_test, yy_predicted)



"""### LASSO"""

from sklearn import linear_model
from sklearn.preprocessing import StandardScaler
import pandas as pd

# Standardizing the features
scaler = StandardScaler()
XX_train_scaled = scaler.fit_transform(XX_train)  # Scale training data

# Fit LassoCV (without 'normalize')
Lasso = linear_model.LassoCV(cv=5, random_state=10, alphas=[0.0005])  # Removed 'normalize' ✅
Lasso.fit(XX_train_scaled, yy_train)

print("Intercept:", Lasso.intercept_)

# Creating DataFrame for coefficients
coef1 = pd.DataFrame(Lasso.coef_, index=XCols, columns=["Value"])

# Display only positive coefficients, sorted by value
print(coef1[coef1["Value"] > 0].sort_values(by="Value", ascending=False))

yy_predicted = Lasso.predict(XX_test)
print(metrics.mean_squared_error(y_true=yy_test, y_pred=yy_predicted))
print(r2_score(yy_test, yy_predicted))

Lasso.fit(XX,yy)



"""## Adaptive Boost"""

from sklearn.ensemble import AdaBoostRegressor
regressor = AdaBoostRegressor(n_estimators=100,loss="linear",learning_rate=.005)

regressor.fit(XX_train, yy_train)

yy_predicted = regressor.predict(XX_test)
print(metrics.mean_squared_error(y_true=yy_test, y_pred=yy_predicted))
print(r2_score(yy_test, yy_predicted))

regressor.fit(XX,yy)



"""### Ridge"""

Ridge = linear_model.Ridge(random_state=10, alpha=0.001)  # Removed 'normalize'

Ridge.fit(XX_train,yy_train)
#print(Ridge.intercept_)
coef1 = pd.DataFrame(Ridge.coef_,XCols,columns=["Value"])
#coef1[coef1["Value"]>0].sort_values(by="Value",ascending=False)

yy_predicted = Ridge.predict(XX_test)
print(metrics.mean_squared_error(y_true=yy_test, y_pred=yy_predicted))
print(r2_score(yy_test, yy_predicted))

Ridge.fit(XX,yy)





"""### Decision tree for Regression"""

from sklearn.tree import DecisionTreeRegressor
Dtree = DecisionTreeRegressor(max_depth=3)
Dtree.fit(XX_train,yy_train)

yy_predicted = Dtree.predict(XX_test)
print(metrics.mean_squared_error(y_true=yy_test, y_pred=yy_predicted))
print(r2_score(yy_test, yy_predicted))

Dtree.fit(XX,yy)



from sklearn.ensemble import RandomForestRegressor
RForrest = RandomForestRegressor()
RForrest.fit(XX_train,yy_train)

yy_predicted = Dtree.predict(XX_test)
print(metrics.mean_squared_error(y_true=yy_test, y_pred=yy_predicted))
print(r2_score(yy_test, yy_predicted))

Dtree.fit(XX,yy)





"""## Testing the MODEL to real world data"""

Sales_Predict = Sales.loc[Sales["Type"]=="test",XCols]
Sales_Predict.head()

"""##### Combining LASSO, ADB and Ridge together"""

Lasso_Prediction = Lasso.predict(Sales_Predict)
ADB_Prediction = regressor.predict(Sales_Predict)
Ridge_Prediction = Ridge.predict(Sales_Predict)
RForrest_Prediction = RForrest.predict(Sales_Predict)
Dtree_Prediction = Dtree.predict(Sales_Predict)

print(Lasso_Prediction[:5])
print(ADB_Prediction[:5])
print(Ridge_Prediction[:5])
print(RForrest_Prediction[:5])
print(Dtree_Prediction[:5])

#Data_Predicted

Data_Predicted["Lasso_Prediction"] = pd.Series(Lasso_Prediction) * Data_Predicted["Item_MRP"]
Data_Predicted["ADB_Prediction"] = pd.Series(ADB_Prediction) * Data_Predicted["Item_MRP"]
Data_Predicted["Ridge_Prediction"] = pd.Series(Ridge_Prediction) * Data_Predicted["Item_MRP"]
Data_Predicted["DTree_Prediction"] = pd.Series(Dtree_Prediction) * Data_Predicted["Item_MRP"]
Data_Predicted["RForrest_Prediction"] = pd.Series(RForrest_Prediction) * Data_Predicted["Item_MRP"]

Data_Predicted["Item_Outlet_Sales"] = (Data_Predicted["Lasso_Prediction"]+Data_Predicted["ADB_Prediction"]+Data_Predicted["ADB_Prediction"]+Data_Predicted["DTree_Prediction"]+Data_Predicted["RForrest_Prediction"])/5

Data_Predicted.drop(["Item_MRP","Lasso_Prediction","ADB_Prediction","Ridge_Prediction","RForrest_Prediction","DTree_Prediction"],axis=1,inplace=True)
Data_Predicted.head()

Data_Predicted.to_csv("SalesPrediction_submission.csv", index=False)



"""### The END!"""